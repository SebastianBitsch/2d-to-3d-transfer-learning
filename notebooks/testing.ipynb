{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "# from tqdm import tqdm\n",
    "import medpy.metric as metric\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "import monai\n",
    "from monai.data import DataLoader, Dataset\n",
    "from monai.transforms.utils import allow_missing_keys_mode\n",
    "from monai.transforms import BatchInverseTransform\n",
    "from monai.networks.nets import DynUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 128, 128, 32])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    DynUNet(\n",
    "        spatial_dims = 3,\n",
    "        in_channels = 1, \n",
    "        out_channels = 2,\n",
    "        kernel_size = [3, 3, 3, 3, 3, 3],\n",
    "        strides = [1, 2, 2, 2, 2, 2], \n",
    "        upsample_kernel_size = [2, 2, 2, 2, 2]\n",
    "    )(\n",
    "        torch.rand((4, 1, 128, 128, 32))\n",
    "    ).shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1, 2, 256, 256, 32]) torch.Size([1, 2, 256, 256, 32]\n",
    "metric.hd95(torch.rand((1, 2, 256, 256, 32)).argmax(dim=1).squeeze(), torch.rand((1, 2, 256, 256, 32)).argmax(dim=1).squeeze(), voxelspacing=[1.464845, 1.464845, 10.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2d = DynUNet(\n",
    "    spatial_dims = 2,\n",
    "    in_channels = 1, \n",
    "    out_channels = 2,\n",
    "    kernel_size = [3, 3, 3, 3, 3, 3],\n",
    "    strides = [1, 2, 2, 2, 2, 2], \n",
    "    upsample_kernel_size = [2, 2, 2, 2, 2]\n",
    ")\n",
    "\n",
    "model3d = DynUNet(\n",
    "    spatial_dims = 3,\n",
    "    in_channels = 1, \n",
    "    out_channels = 2,\n",
    "    kernel_size = [3, 3, 3, 3, 3, 3],\n",
    "    strides = [1, 2, 2, 2, 2, 2], \n",
    "    upsample_kernel_size = [2, 2, 2, 2, 2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 114\n"
     ]
    }
   ],
   "source": [
    "# They have the same number of modules\n",
    "print(len(list(model2d.modules())), len(list(model3d.modules())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2d conv layer shapes\t3d conv layers shapes\n",
      "easy (32, 1, 3, 3) \t (32, 1, 3, 3, 3)\n",
      "easy (32, 32, 3, 3) \t (32, 32, 3, 3, 3)\n",
      "easy (64, 32, 3, 3) \t (64, 32, 3, 3, 3)\n",
      "easy (64, 64, 3, 3) \t (64, 64, 3, 3, 3)\n",
      "easy (128, 64, 3, 3) \t (128, 64, 3, 3, 3)\n",
      "easy (128, 128, 3, 3) \t (128, 128, 3, 3, 3)\n",
      "easy (256, 128, 3, 3) \t (256, 128, 3, 3, 3)\n",
      "easy (256, 256, 3, 3) \t (256, 256, 3, 3, 3)\n",
      "???? (512, 256, 3, 3) \t (320, 256, 3, 3, 3)\n",
      "???? (512, 512, 3, 3) \t (320, 320, 3, 3, 3)\n",
      "???? (512, 512, 3, 3) \t (320, 320, 3, 3, 3)\n",
      "???? (512, 512, 3, 3) \t (320, 320, 3, 3, 3)\n",
      "???? (512, 1024, 3, 3) \t (320, 640, 3, 3, 3)\n",
      "???? (512, 512, 3, 3) \t (320, 320, 3, 3, 3)\n",
      "easy (256, 512, 3, 3) \t (256, 512, 3, 3, 3)\n",
      "easy (256, 256, 3, 3) \t (256, 256, 3, 3, 3)\n",
      "easy (128, 256, 3, 3) \t (128, 256, 3, 3, 3)\n",
      "easy (128, 128, 3, 3) \t (128, 128, 3, 3, 3)\n",
      "easy (64, 128, 3, 3) \t (64, 128, 3, 3, 3)\n",
      "easy (64, 64, 3, 3) \t (64, 64, 3, 3, 3)\n",
      "easy (32, 64, 3, 3) \t (32, 64, 3, 3, 3)\n",
      "easy (32, 32, 3, 3) \t (32, 32, 3, 3, 3)\n",
      "easy (2, 32, 1, 1) \t (2, 32, 1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"2d conv layer shapes\\t3d conv layers shapes\")\n",
    "for mod2d, mod3d in zip(model2d.modules(), model3d.modules()):\n",
    "    if isinstance(mod2d, nn.Conv2d) and isinstance(mod3d, nn.Conv3d):\n",
    "        weights2d = mod2d.weight.data.numpy().shape\n",
    "        weights3d = mod3d.weight.data.numpy().shape\n",
    "        state = \"easy\" if weights2d[:2] == weights3d[:2] else \"????\"\n",
    "        print(state, weights2d, \"\\t\", weights3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6326, -0.8669, -0.9719],\n",
       "         [ 2.2894,  0.6494, -0.7566],\n",
       "         [-0.7648,  0.1312,  0.4432]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(32, 1, 3, 3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init_weights_from_distribution_of_mode():\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2d all layer shapes\t\t\t 3d all layers shapes\n",
      "easy Conv2d (32, 1, 3, 3) \t\t Conv3d (32, 1, 3, 3, 3)\n",
      "easy Conv2d (32, 32, 3, 3) \t\t Conv3d (32, 32, 3, 3, 3)\n",
      "easy InstanceNorm2d (32,) \t\t InstanceNorm3d (32,)\n",
      "easy InstanceNorm2d (32,) \t\t InstanceNorm3d (32,)\n",
      "easy Conv2d (64, 32, 3, 3) \t\t Conv3d (64, 32, 3, 3, 3)\n",
      "easy Conv2d (64, 64, 3, 3) \t\t Conv3d (64, 64, 3, 3, 3)\n",
      "easy InstanceNorm2d (64,) \t\t InstanceNorm3d (64,)\n",
      "easy InstanceNorm2d (64,) \t\t InstanceNorm3d (64,)\n",
      "easy Conv2d (128, 64, 3, 3) \t\t Conv3d (128, 64, 3, 3, 3)\n",
      "easy Conv2d (128, 128, 3, 3) \t\t Conv3d (128, 128, 3, 3, 3)\n",
      "easy InstanceNorm2d (128,) \t\t InstanceNorm3d (128,)\n",
      "easy InstanceNorm2d (128,) \t\t InstanceNorm3d (128,)\n",
      "easy Conv2d (256, 128, 3, 3) \t\t Conv3d (256, 128, 3, 3, 3)\n",
      "easy Conv2d (256, 256, 3, 3) \t\t Conv3d (256, 256, 3, 3, 3)\n",
      "easy InstanceNorm2d (256,) \t\t InstanceNorm3d (256,)\n",
      "easy InstanceNorm2d (256,) \t\t InstanceNorm3d (256,)\n",
      "???? Conv2d (512, 256, 3, 3) \t\t Conv3d (320, 256, 3, 3, 3)\n",
      "???? Conv2d (512, 512, 3, 3) \t\t Conv3d (320, 320, 3, 3, 3)\n",
      "???? InstanceNorm2d (512,) \t\t InstanceNorm3d (320,)\n",
      "???? InstanceNorm2d (512,) \t\t InstanceNorm3d (320,)\n",
      "???? Conv2d (512, 512, 3, 3) \t\t Conv3d (320, 320, 3, 3, 3)\n",
      "???? Conv2d (512, 512, 3, 3) \t\t Conv3d (320, 320, 3, 3, 3)\n",
      "???? InstanceNorm2d (512,) \t\t InstanceNorm3d (320,)\n",
      "???? InstanceNorm2d (512,) \t\t InstanceNorm3d (320,)\n",
      "???? ConvTranspose2d (512, 512, 2, 2) \t\t ConvTranspose3d (320, 320, 2, 2, 2)\n",
      "???? Conv2d (512, 1024, 3, 3) \t\t Conv3d (320, 640, 3, 3, 3)\n",
      "???? Conv2d (512, 512, 3, 3) \t\t Conv3d (320, 320, 3, 3, 3)\n",
      "???? InstanceNorm2d (512,) \t\t InstanceNorm3d (320,)\n",
      "???? InstanceNorm2d (512,) \t\t InstanceNorm3d (320,)\n",
      "???? ConvTranspose2d (512, 256, 2, 2) \t\t ConvTranspose3d (320, 256, 2, 2, 2)\n",
      "easy Conv2d (256, 512, 3, 3) \t\t Conv3d (256, 512, 3, 3, 3)\n",
      "easy Conv2d (256, 256, 3, 3) \t\t Conv3d (256, 256, 3, 3, 3)\n",
      "easy InstanceNorm2d (256,) \t\t InstanceNorm3d (256,)\n",
      "easy InstanceNorm2d (256,) \t\t InstanceNorm3d (256,)\n",
      "easy ConvTranspose2d (256, 128, 2, 2) \t\t ConvTranspose3d (256, 128, 2, 2, 2)\n",
      "easy Conv2d (128, 256, 3, 3) \t\t Conv3d (128, 256, 3, 3, 3)\n",
      "easy Conv2d (128, 128, 3, 3) \t\t Conv3d (128, 128, 3, 3, 3)\n",
      "easy InstanceNorm2d (128,) \t\t InstanceNorm3d (128,)\n",
      "easy InstanceNorm2d (128,) \t\t InstanceNorm3d (128,)\n",
      "easy ConvTranspose2d (128, 64, 2, 2) \t\t ConvTranspose3d (128, 64, 2, 2, 2)\n",
      "easy Conv2d (64, 128, 3, 3) \t\t Conv3d (64, 128, 3, 3, 3)\n",
      "easy Conv2d (64, 64, 3, 3) \t\t Conv3d (64, 64, 3, 3, 3)\n",
      "easy InstanceNorm2d (64,) \t\t InstanceNorm3d (64,)\n",
      "easy InstanceNorm2d (64,) \t\t InstanceNorm3d (64,)\n",
      "easy ConvTranspose2d (64, 32, 2, 2) \t\t ConvTranspose3d (64, 32, 2, 2, 2)\n",
      "easy Conv2d (32, 64, 3, 3) \t\t Conv3d (32, 64, 3, 3, 3)\n",
      "easy Conv2d (32, 32, 3, 3) \t\t Conv3d (32, 32, 3, 3, 3)\n",
      "easy InstanceNorm2d (32,) \t\t InstanceNorm3d (32,)\n",
      "easy InstanceNorm2d (32,) \t\t InstanceNorm3d (32,)\n",
      "easy Conv2d (2, 32, 1, 1) \t\t Conv3d (2, 32, 1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"2d all layer shapes\\t\\t\\t 3d all layers shapes\")\n",
    "for mod2d, mod3d in zip(model2d.modules(), model3d.modules()):\n",
    "    try:\n",
    "        weights2d = mod2d.weight.data.numpy().shape\n",
    "        weights3d = mod3d.weight.data.numpy().shape\n",
    "        state = \"easy\" if weights2d[:2] == weights3d[:2] else \"????\"\n",
    "        print(state, mod2d.__class__.__name__, weights2d, \"\\t\\t\", mod3d.__class__.__name__, weights3d)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DynUNet(\n",
      "  (input_block): UnetBasicBlock(\n",
      "    (conv1): Convolution(\n",
      "      (conv): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (conv2): Convolution(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (norm1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (norm2): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "  )\n",
      "  (downsamples): ModuleList(\n",
      "    (0): UnetBasicBlock(\n",
      "      (conv1): Convolution(\n",
      "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (conv2): Convolution(\n",
      "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (norm1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (norm2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (1): UnetBasicBlock(\n",
      "      (conv1): Convolution(\n",
      "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (conv2): Convolution(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (norm1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (2): UnetBasicBlock(\n",
      "      (conv1): Convolution(\n",
      "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (conv2): Convolution(\n",
      "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (norm1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (norm2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (3): UnetBasicBlock(\n",
      "      (conv1): Convolution(\n",
      "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (conv2): Convolution(\n",
      "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (norm1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (norm2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "  )\n",
      "  (bottleneck): UnetBasicBlock(\n",
      "    (conv1): Convolution(\n",
      "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (conv2): Convolution(\n",
      "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (norm1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (norm2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "  )\n",
      "  (upsamples): ModuleList(\n",
      "    (0): UnetUpBlock(\n",
      "      (transp_conv): Convolution(\n",
      "        (conv): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "      )\n",
      "      (conv_block): UnetBasicBlock(\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        (norm1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "        (norm2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (1): UnetUpBlock(\n",
      "      (transp_conv): Convolution(\n",
      "        (conv): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "      )\n",
      "      (conv_block): UnetBasicBlock(\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        (norm1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "        (norm2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (2): UnetUpBlock(\n",
      "      (transp_conv): Convolution(\n",
      "        (conv): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "      )\n",
      "      (conv_block): UnetBasicBlock(\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        (norm1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "        (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (3): UnetUpBlock(\n",
      "      (transp_conv): Convolution(\n",
      "        (conv): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "      )\n",
      "      (conv_block): UnetBasicBlock(\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        (norm1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "        (norm2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (4): UnetUpBlock(\n",
      "      (transp_conv): Convolution(\n",
      "        (conv): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "      )\n",
      "      (conv_block): UnetBasicBlock(\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        (norm1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "        (norm2): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_block): UnetOutBlock(\n",
      "    (conv): Convolution(\n",
      "      (conv): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (skip_layers): DynUNetSkipLayer(\n",
      "    (downsample): UnetBasicBlock(\n",
      "      (conv1): Convolution(\n",
      "        (conv): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (conv2): Convolution(\n",
      "        (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (norm1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (norm2): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (next_layer): DynUNetSkipLayer(\n",
      "      (downsample): UnetBasicBlock(\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        (norm1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "        (norm2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      )\n",
      "      (next_layer): DynUNetSkipLayer(\n",
      "        (downsample): UnetBasicBlock(\n",
      "          (conv1): Convolution(\n",
      "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          )\n",
      "          (conv2): Convolution(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          )\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          (norm1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "        )\n",
      "        (next_layer): DynUNetSkipLayer(\n",
      "          (downsample): UnetBasicBlock(\n",
      "            (conv1): Convolution(\n",
      "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            )\n",
      "            (conv2): Convolution(\n",
      "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            )\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "            (norm1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (norm2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          )\n",
      "          (next_layer): DynUNetSkipLayer(\n",
      "            (downsample): UnetBasicBlock(\n",
      "              (conv1): Convolution(\n",
      "                (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              )\n",
      "              (conv2): Convolution(\n",
      "                (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              )\n",
      "              (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "              (norm1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "              (norm2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            )\n",
      "            (next_layer): UnetBasicBlock(\n",
      "              (conv1): Convolution(\n",
      "                (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              )\n",
      "              (conv2): Convolution(\n",
      "                (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              )\n",
      "              (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "              (norm1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "              (norm2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            )\n",
      "            (upsample): UnetUpBlock(\n",
      "              (transp_conv): Convolution(\n",
      "                (conv): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "              )\n",
      "              (conv_block): UnetBasicBlock(\n",
      "                (conv1): Convolution(\n",
      "                  (conv): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (conv2): Convolution(\n",
      "                  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "                (norm1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "                (norm2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (upsample): UnetUpBlock(\n",
      "            (transp_conv): Convolution(\n",
      "              (conv): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "            )\n",
      "            (conv_block): UnetBasicBlock(\n",
      "              (conv1): Convolution(\n",
      "                (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              )\n",
      "              (conv2): Convolution(\n",
      "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              )\n",
      "              (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "              (norm1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "              (norm2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (upsample): UnetUpBlock(\n",
      "          (transp_conv): Convolution(\n",
      "            (conv): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "          )\n",
      "          (conv_block): UnetBasicBlock(\n",
      "            (conv1): Convolution(\n",
      "              (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            )\n",
      "            (conv2): Convolution(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            )\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "            (norm1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (upsample): UnetUpBlock(\n",
      "        (transp_conv): Convolution(\n",
      "          (conv): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "        )\n",
      "        (conv_block): UnetBasicBlock(\n",
      "          (conv1): Convolution(\n",
      "            (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          )\n",
      "          (conv2): Convolution(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          )\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          (norm1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (norm2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (upsample): UnetUpBlock(\n",
      "      (transp_conv): Convolution(\n",
      "        (conv): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "      )\n",
      "      (conv_block): UnetBasicBlock(\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        (norm1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "        (norm2): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_normal_per_module(from_module: nn.Module, to_module: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Initialize the weights of a model (to_module) using the normal distributions of the layers in another module (from_module).\n",
    "    The function will take the weights in an entire module i.e. shape [128, 64, 3, 3] and compute a single number mean and std to \n",
    "    init the entire layer in another module.\n",
    "    \n",
    "    Args:\n",
    "        from_module (nn.Module): Source module providing weights for initialization.\n",
    "        to_module (nn.Module): Target module whose weights will be initialized.\n",
    "    \n",
    "    Note:\n",
    "        The modules should be -very- similar as this functions makes the assumption the models have the same layers (either 2D or 3D) in the same order.\n",
    "    \"\"\"\n",
    "    assert len(list(from_module.modules())) == len(list(to_module.modules())), \"Error: Models should contain the 'same' layers\"\n",
    "\n",
    "    for from_mod, to_mod in zip(from_module.modules(), to_module.modules()):\n",
    "\n",
    "        if isinstance(from_mod, (nn.Conv2d, nn.InstanceNorm2d, nn.ConvTranspose2d)):\n",
    "            # Get distributions across the entire module, i.e. a single number for mean and std\n",
    "            mean = torch.mean(from_mod.weight.data) \n",
    "            std  = torch.std(from_mod.weight.data)\n",
    "            \n",
    "            to_mod.weight.data.normal_(mean, std)\n",
    "\n",
    "        elif isinstance(from_mod, nn.LeakyReLU):\n",
    "            to_mod.negative_slope = from_mod.negative_slope\n",
    "\n",
    "init_normal_per_module(model2d, model3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_normal_per_layer(from_module: nn.Module, to_module: nn.Module) -> None:\n",
    "    pass\n",
    "\n",
    "init_normal_per_layer(model2d, model3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_mod.weight.data = torch.ones(to_mod.weight.data.shape)#torch.normal(mean.expand(weight_shape), std.expand(weight_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init from \"layer-wise\"\n",
    "a = torch.randn([32, 32, 3, 3])\n",
    "torch.mean(a, axis=[0,1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]]])\n"
     ]
    }
   ],
   "source": [
    "for mod in model3d.modules():\n",
    "    if isinstance(mod, (nn.Conv3d, nn.InstanceNorm3d, nn.ConvTranspose3d)):\n",
    "        print(mod.weight.data)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5979])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(10,1)\n",
    "torch.normal(torch.zeros(1), std = torch.ones(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(1).expand(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
