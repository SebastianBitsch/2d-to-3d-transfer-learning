{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "# from tqdm import tqdm\n",
    "import medpy.metric as metric\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "import monai\n",
    "from monai.data import DataLoader, Dataset\n",
    "from monai.transforms.utils import allow_missing_keys_mode\n",
    "from monai.transforms import BatchInverseTransform\n",
    "from monai.networks.nets import DynUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 128, 128, 32])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    DynUNet(\n",
    "        spatial_dims = 3,\n",
    "        in_channels = 1, \n",
    "        out_channels = 2,\n",
    "        kernel_size = [3, 3, 3, 3, 3, 3],\n",
    "        strides = [1, 2, 2, 2, 2, 2], \n",
    "        upsample_kernel_size = [2, 2, 2, 2, 2]\n",
    "    )(\n",
    "        torch.rand((4, 1, 128, 128, 32))\n",
    "    ).shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1, 2, 256, 256, 32]) torch.Size([1, 2, 256, 256, 32]\n",
    "metric.hd95(torch.rand((1, 2, 256, 256, 32)).argmax(dim=1).squeeze(), torch.rand((1, 2, 256, 256, 32)).argmax(dim=1).squeeze(), voxelspacing=[1.464845, 1.464845, 10.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2d = DynUNet(\n",
    "    spatial_dims = 2,\n",
    "    in_channels = 1, \n",
    "    out_channels = 2,\n",
    "    kernel_size = [3, 3, 3, 3, 3, 3],\n",
    "    strides = [1, 2, 2, 2, 2, 2], \n",
    "    upsample_kernel_size = [2, 2, 2, 2, 2]\n",
    ")\n",
    "\n",
    "model3d = DynUNet(\n",
    "    spatial_dims = 3,\n",
    "    in_channels = 1, \n",
    "    out_channels = 2,\n",
    "    kernel_size = [3, 3, 3, 3, 3, 3],\n",
    "    strides = [1, 2, 2, 2, 2, 2], \n",
    "    upsample_kernel_size = [2, 2, 2, 2, 2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 114\n"
     ]
    }
   ],
   "source": [
    "# They have the same number of modules\n",
    "print(len(list(model2d.modules())), len(list(model3d.modules())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2d conv layer shapes\t3d conv layers shapes\n",
      "easy (32, 1, 3, 3) \t (32, 1, 3, 3, 3)\n",
      "easy (32, 32, 3, 3) \t (32, 32, 3, 3, 3)\n",
      "easy (64, 32, 3, 3) \t (64, 32, 3, 3, 3)\n",
      "easy (64, 64, 3, 3) \t (64, 64, 3, 3, 3)\n",
      "easy (128, 64, 3, 3) \t (128, 64, 3, 3, 3)\n",
      "easy (128, 128, 3, 3) \t (128, 128, 3, 3, 3)\n",
      "easy (256, 128, 3, 3) \t (256, 128, 3, 3, 3)\n",
      "easy (256, 256, 3, 3) \t (256, 256, 3, 3, 3)\n",
      "easy (320, 256, 3, 3) \t (320, 256, 3, 3, 3)\n",
      "easy (320, 320, 3, 3) \t (320, 320, 3, 3, 3)\n",
      "easy (320, 320, 3, 3) \t (320, 320, 3, 3, 3)\n",
      "easy (320, 320, 3, 3) \t (320, 320, 3, 3, 3)\n",
      "easy (320, 640, 3, 3) \t (320, 640, 3, 3, 3)\n",
      "easy (320, 320, 3, 3) \t (320, 320, 3, 3, 3)\n",
      "easy (256, 512, 3, 3) \t (256, 512, 3, 3, 3)\n",
      "easy (256, 256, 3, 3) \t (256, 256, 3, 3, 3)\n",
      "easy (128, 256, 3, 3) \t (128, 256, 3, 3, 3)\n",
      "easy (128, 128, 3, 3) \t (128, 128, 3, 3, 3)\n",
      "easy (64, 128, 3, 3) \t (64, 128, 3, 3, 3)\n",
      "easy (64, 64, 3, 3) \t (64, 64, 3, 3, 3)\n",
      "easy (32, 64, 3, 3) \t (32, 64, 3, 3, 3)\n",
      "easy (32, 32, 3, 3) \t (32, 32, 3, 3, 3)\n",
      "easy (2, 32, 1, 1) \t (2, 32, 1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"2d conv layer shapes\\t3d conv layers shapes\")\n",
    "for mod2d, mod3d in zip(model2d.modules(), model3d.modules()):\n",
    "    if (isinstance(mod2d, nn.Conv2d) and isinstance(mod3d, nn.Conv3d)):\n",
    "        weights2d = mod2d.weight.data.numpy().shape\n",
    "        weights3d = mod3d.weight.data.numpy().shape\n",
    "        state = \"easy\" if weights2d[:2] == weights3d[:2] else \"????\"\n",
    "        print(state, weights2d, \"\\t\", weights3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2d conv.T layer shapes\t3d conv.T layer shapes\n",
      "easy (320, 320, 2, 2) \t (320, 320, 2, 2, 2)\n",
      "easy (320, 256, 2, 2) \t (320, 256, 2, 2, 2)\n",
      "easy (256, 128, 2, 2) \t (256, 128, 2, 2, 2)\n",
      "easy (128, 64, 2, 2) \t (128, 64, 2, 2, 2)\n",
      "easy (64, 32, 2, 2) \t (64, 32, 2, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"2d conv.T layer shapes\\t3d conv.T layer shapes\")\n",
    "for mod2d, mod3d in zip(model2d.modules(), model3d.modules()):\n",
    "    if (isinstance(mod2d, nn.ConvTranspose2d) and isinstance(mod3d, nn.ConvTranspose3d)):\n",
    "        weights2d = mod2d.weight.data.numpy().shape\n",
    "        weights3d = mod3d.weight.data.numpy().shape\n",
    "        state = \"easy\" if weights2d[:2] == weights3d[:2] else \"????\"\n",
    "        print(state, weights2d, \"\\t\", weights3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_normal_per_channel(from_module: nn.Module, to_module: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Initialize the weights of a model (to_module) using the normal distributions per conv channel in another module (from_module).\n",
    "    The function will take the weights in an entire module i.e. shape [128, 64, 3, 3] and compute a single number mean and std with \n",
    "    shape [128, 64, 1], and tile it to shape [128, 64, 3, 3, 3]\n",
    "    \n",
    "    Args:\n",
    "        from_module (nn.Module): Source module providing weights for initialization.\n",
    "        to_module (nn.Module): Target module whose weights will be initialized.\n",
    "    \n",
    "    Note:\n",
    "        The modules should be -very- similar as this functions makes the assumption the models have the same layers (either 2D or 3D) in the same order.\n",
    "    \"\"\"\n",
    "    assert len(list(from_module.modules())) == len(list(to_module.modules())), \"Error: Models should contain the 'same' layers\"\n",
    "\n",
    "    for from_mod, to_mod in zip(from_module.modules(), to_module.modules()):\n",
    "\n",
    "        if isinstance(from_mod, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "\n",
    "            # Handle the 'complicated' case where shapes dont match, for instance (512, 256, 3, 3) and (320, 256, 3, 3, 3).\n",
    "            # TODO: We could try something more complicated, e.g. interpolation, random crop, tiling etc. for now we just take a simple crop\n",
    "            if from_mod.weight.shape != to_mod.weight.shape[:4]:\n",
    "                from_mod.weight.data = from_mod.weight.data[:to_mod.weight.shape[0], :to_mod.weight.shape[1]]\n",
    "\n",
    "            # This ungodly line takes the mean of the 2D conv (3,3) and then expands the single number into a 3D convolution, i.e. [32, 64] -> [32, 64, 3, 3, 3]\n",
    "            # TODO: I cant find a nicer way than the triple unsqueeze, though i am sure there is a nicer / more general way of doing it\n",
    "            mean = torch.mean(from_mod.weight.data, axis = (-1, -2)).unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(to_mod.weight.data.shape)\n",
    "            std  = torch.std(from_mod.weight.data, axis = (-1, -2), unbiased=False).unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(to_mod.weight.data.shape) # Unbiased, see here: https://github.com/pytorch/pytorch/issues/29372\n",
    "            \n",
    "            # Handle Conv2D layers on the form [2, 32, 1, 1]. They have only 1 number so std is 'nan', replace with 0\n",
    "            std = torch.nan_to_num(std, nan = 0.0)\n",
    "\n",
    "            to_mod.weight.data = torch.normal(mean, std = std)\n",
    "                        \n",
    "        elif isinstance(from_mod, nn.LeakyReLU):\n",
    "            to_mod.negative_slope = from_mod.negative_slope\n",
    "\n",
    "        elif isinstance(from_mod, nn.InstanceNorm2d):\n",
    "            to_mod.weight.data = from_mod.weight.data\n",
    "\n",
    "\n",
    "saved_model = torch.load(\"/work3/s204163/3dimaging_finalproject/weights/baseline2d_101/baseline2d_final.pt\", map_location=torch.device('cpu'))\n",
    "init_normal_per_channel(from_module=saved_model, to_module=model3d)\n",
    "\n",
    "# init_normal_per_channel(from_module=model2d, to_module=model3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10, 3, 3])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn([32, 32, 3, 3])[:10, :10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.mean(torch.randn([2, 32, 3, 3]), axis=(-1,-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32, 3, 3, 3])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(2, 32, 3, 3, 3)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5448)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5448, 0.5448, 0.5448],\n",
       "         [0.5448, 0.5448, 0.5448],\n",
       "         [0.5448, 0.5448, 0.5448]],\n",
       "\n",
       "        [[0.5448, 0.5448, 0.5448],\n",
       "         [0.5448, 0.5448, 0.5448],\n",
       "         [0.5448, 0.5448, 0.5448]],\n",
       "\n",
       "        [[0.5448, 0.5448, 0.5448],\n",
       "         [0.5448, 0.5448, 0.5448],\n",
       "         [0.5448, 0.5448, 0.5448]]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5000],\n",
       "        [ 5.5000],\n",
       "        [ 9.5000],\n",
       "        [13.5000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.arange(16,dtype=float).reshape(4, 1, 2, 2), axis=(-1, -2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(torch.zeros(4), std = torch.ones(4)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 320, 256])\n",
      "torch.Size([320, 256, 3])\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have your image tensor\n",
    "image_original = torch.randn(512, 256, 3)\n",
    "\n",
    "# Resize the image tensor using bilinear interpolation\n",
    "image_resized = F.interpolate(image_original.unsqueeze(0).permute(0, 3, 1, 2), size=(320, 256), mode='bilinear', align_corners=False)\n",
    "print(image_resized.shape)\n",
    "# Remove the batch dimension and adjust the shape\n",
    "image_resized = image_resized.squeeze(0).permute(1, 2, 0)\n",
    "\n",
    "print(image_resized.shape)  # Output: torch.Size([320, 256, 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Got 5D input, but bilinear mode needs 4D input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[173], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m      7\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbilinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      9\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     10\u001b[0m x\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m/dtu/3d-imaging-center/courses/conda/miniconda3/envs/env-02510/lib/python3.11/site-packages/torch/nn/functional.py:4041\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   4039\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot 5D input, but linear mode needs 3D input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 4041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot 5D input, but bilinear mode needs 4D input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4043\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   4044\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput Error: Only 3D, 4D and 5D input Tensors supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4045\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD) for the modes: nearest | linear | bilinear | bicubic | trilinear | area | nearest-exact\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4046\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4047\u001b[0m )\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Got 5D input, but bilinear mode needs 4D input"
     ]
    }
   ],
   "source": [
    "# x = torch.ones(512, 256, 3, 3)\n",
    "\n",
    "# x = F.interpolate(x.unsqueeze(0), size=(3,4,4), mode=\"trilinear\").squeeze(0)\n",
    "# x.shape\n",
    "\n",
    "x = torch.ones(3,4,64,64)\n",
    "x = x.permute(1,0,2,3)\n",
    "x = F.interpolate(x.unsqueeze(0), size=(3, 2, 10), mode=\"bilinear\").squeeze(0)\n",
    "x = x.permute(1,0,2,3)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Input Error: Only 3D, 4D and 5D input Tensors supported (got 6D) for the modes: nearest | linear | bilinear | bicubic | trilinear | area | nearest-exact (got trilinear)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[134], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m weights_reshaped \u001b[38;5;241m=\u001b[39m weights_original\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Resize the weights tensor using trilinear interpolation\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m weights_resized \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m320\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrilinear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Remove the singleton dimensions\u001b[39;00m\n\u001b[1;32m     17\u001b[0m weights_resized \u001b[38;5;241m=\u001b[39m weights_resized\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/dtu/3d-imaging-center/courses/conda/miniconda3/envs/env-02510/lib/python3.11/site-packages/torch/nn/functional.py:4043\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   4040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   4041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot 5D input, but bilinear mode needs 4D input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 4043\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   4044\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput Error: Only 3D, 4D and 5D input Tensors supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4045\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD) for the modes: nearest | linear | bilinear | bicubic | trilinear | area | nearest-exact\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4046\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4047\u001b[0m )\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Input Error: Only 3D, 4D and 5D input Tensors supported (got 6D) for the modes: nearest | linear | bilinear | bicubic | trilinear | area | nearest-exact (got trilinear)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Assuming you have your weights tensor\n",
    "weights_original = torch.randn(512, 256, 3, 3)\n",
    "\n",
    "# Reshape the original weights tensor to add a singleton dimension\n",
    "# This is required to match the spatial dimensions for trilinear interpolation\n",
    "weights_reshaped = weights_original.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# Resize the weights tensor using trilinear interpolation\n",
    "weights_resized = F.interpolate(weights_reshaped, size=(320, 256, 3, 3), mode='trilinear', align_corners=False)\n",
    "\n",
    "# Remove the singleton dimensions\n",
    "weights_resized = weights_resized.squeeze(0).squeeze(0)\n",
    "\n",
    "# Plot the original weights\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax1.set_title('Original Weights')\n",
    "\n",
    "x, y, z = torch.meshgrid(torch.arange(weights_original.shape[0]),\n",
    "                          torch.arange(weights_original.shape[1]),\n",
    "                          torch.arange(weights_original.shape[2] * 3),\n",
    "                          indexing='ij')\n",
    "\n",
    "ax1.scatter(x, y, z, c=weights_original.view(-1).numpy(), cmap='viridis')\n",
    "ax1.set_xlabel('Dimension 1')\n",
    "ax1.set_ylabel('Dimension 2')\n",
    "ax1.set_zlabel('Dimension 3')\n",
    "\n",
    "# Plot the resized weights\n",
    "ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "ax2.set_title('Resized Weights')\n",
    "\n",
    "x, y, z = torch.meshgrid(torch.arange(weights_resized.shape[0]),\n",
    "                          torch.arange(weights_resized.shape[1]),\n",
    "                          torch.arange(weights_resized.shape[2] * 3),\n",
    "                          indexing='ij')\n",
    "\n",
    "ax2.scatter(x, y, z, c=weights_resized.view(-1).numpy(), cmap='viridis')\n",
    "ax2.set_xlabel('Dimension 1')\n",
    "ax2.set_ylabel('Dimension 2')\n",
    "ax2.set_zlabel('Dimension 3')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init_weights_from_distribution_of_mode():\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2d all layer shapes\t\t\t 3d all layers shapes\n",
      "easy Conv2d (32, 1, 3, 3) \t\t Conv3d (32, 1, 3, 3, 3)\n",
      "easy Conv2d (32, 32, 3, 3) \t\t Conv3d (32, 32, 3, 3, 3)\n",
      "easy InstanceNorm2d (32,) \t\t InstanceNorm3d (32,)\n",
      "easy InstanceNorm2d (32,) \t\t InstanceNorm3d (32,)\n",
      "easy Conv2d (64, 32, 3, 3) \t\t Conv3d (64, 32, 3, 3, 3)\n",
      "easy Conv2d (64, 64, 3, 3) \t\t Conv3d (64, 64, 3, 3, 3)\n",
      "easy InstanceNorm2d (64,) \t\t InstanceNorm3d (64,)\n",
      "easy InstanceNorm2d (64,) \t\t InstanceNorm3d (64,)\n",
      "easy Conv2d (128, 64, 3, 3) \t\t Conv3d (128, 64, 3, 3, 3)\n",
      "easy Conv2d (128, 128, 3, 3) \t\t Conv3d (128, 128, 3, 3, 3)\n",
      "easy InstanceNorm2d (128,) \t\t InstanceNorm3d (128,)\n",
      "easy InstanceNorm2d (128,) \t\t InstanceNorm3d (128,)\n",
      "easy Conv2d (256, 128, 3, 3) \t\t Conv3d (256, 128, 3, 3, 3)\n",
      "easy Conv2d (256, 256, 3, 3) \t\t Conv3d (256, 256, 3, 3, 3)\n",
      "easy InstanceNorm2d (256,) \t\t InstanceNorm3d (256,)\n",
      "easy InstanceNorm2d (256,) \t\t InstanceNorm3d (256,)\n",
      "???? Conv2d (512, 256, 3, 3) \t\t Conv3d (320, 256, 3, 3, 3)\n",
      "???? Conv2d (512, 512, 3, 3) \t\t Conv3d (320, 320, 3, 3, 3)\n",
      "???? InstanceNorm2d (512,) \t\t InstanceNorm3d (320,)\n",
      "???? InstanceNorm2d (512,) \t\t InstanceNorm3d (320,)\n",
      "???? Conv2d (512, 512, 3, 3) \t\t Conv3d (320, 320, 3, 3, 3)\n",
      "???? Conv2d (512, 512, 3, 3) \t\t Conv3d (320, 320, 3, 3, 3)\n",
      "???? InstanceNorm2d (512,) \t\t InstanceNorm3d (320,)\n",
      "???? InstanceNorm2d (512,) \t\t InstanceNorm3d (320,)\n",
      "???? ConvTranspose2d (512, 512, 2, 2) \t\t ConvTranspose3d (320, 320, 2, 2, 2)\n",
      "???? Conv2d (512, 1024, 3, 3) \t\t Conv3d (320, 640, 3, 3, 3)\n",
      "???? Conv2d (512, 512, 3, 3) \t\t Conv3d (320, 320, 3, 3, 3)\n",
      "???? InstanceNorm2d (512,) \t\t InstanceNorm3d (320,)\n",
      "???? InstanceNorm2d (512,) \t\t InstanceNorm3d (320,)\n",
      "???? ConvTranspose2d (512, 256, 2, 2) \t\t ConvTranspose3d (320, 256, 2, 2, 2)\n",
      "easy Conv2d (256, 512, 3, 3) \t\t Conv3d (256, 512, 3, 3, 3)\n",
      "easy Conv2d (256, 256, 3, 3) \t\t Conv3d (256, 256, 3, 3, 3)\n",
      "easy InstanceNorm2d (256,) \t\t InstanceNorm3d (256,)\n",
      "easy InstanceNorm2d (256,) \t\t InstanceNorm3d (256,)\n",
      "easy ConvTranspose2d (256, 128, 2, 2) \t\t ConvTranspose3d (256, 128, 2, 2, 2)\n",
      "easy Conv2d (128, 256, 3, 3) \t\t Conv3d (128, 256, 3, 3, 3)\n",
      "easy Conv2d (128, 128, 3, 3) \t\t Conv3d (128, 128, 3, 3, 3)\n",
      "easy InstanceNorm2d (128,) \t\t InstanceNorm3d (128,)\n",
      "easy InstanceNorm2d (128,) \t\t InstanceNorm3d (128,)\n",
      "easy ConvTranspose2d (128, 64, 2, 2) \t\t ConvTranspose3d (128, 64, 2, 2, 2)\n",
      "easy Conv2d (64, 128, 3, 3) \t\t Conv3d (64, 128, 3, 3, 3)\n",
      "easy Conv2d (64, 64, 3, 3) \t\t Conv3d (64, 64, 3, 3, 3)\n",
      "easy InstanceNorm2d (64,) \t\t InstanceNorm3d (64,)\n",
      "easy InstanceNorm2d (64,) \t\t InstanceNorm3d (64,)\n",
      "easy ConvTranspose2d (64, 32, 2, 2) \t\t ConvTranspose3d (64, 32, 2, 2, 2)\n",
      "easy Conv2d (32, 64, 3, 3) \t\t Conv3d (32, 64, 3, 3, 3)\n",
      "easy Conv2d (32, 32, 3, 3) \t\t Conv3d (32, 32, 3, 3, 3)\n",
      "easy InstanceNorm2d (32,) \t\t InstanceNorm3d (32,)\n",
      "easy InstanceNorm2d (32,) \t\t InstanceNorm3d (32,)\n",
      "easy Conv2d (2, 32, 1, 1) \t\t Conv3d (2, 32, 1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"2d all layer shapes\\t\\t\\t 3d all layers shapes\")\n",
    "for mod2d, mod3d in zip(model2d.modules(), model3d.modules()):\n",
    "    try:\n",
    "        weights2d = mod2d.weight.data.numpy().shape\n",
    "        weights3d = mod3d.weight.data.numpy().shape\n",
    "        state = \"easy\" if weights2d[:2] == weights3d[:2] else \"????\"\n",
    "        print(state, mod2d.__class__.__name__, weights2d, \"\\t\\t\", mod3d.__class__.__name__, weights3d)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DynUNet(\n",
      "  (input_block): UnetBasicBlock(\n",
      "    (conv1): Convolution(\n",
      "      (conv): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (conv2): Convolution(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (norm1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (norm2): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "  )\n",
      "  (downsamples): ModuleList(\n",
      "    (0): UnetBasicBlock(\n",
      "      (conv1): Convolution(\n",
      "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (conv2): Convolution(\n",
      "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (norm1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (norm2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (1): UnetBasicBlock(\n",
      "      (conv1): Convolution(\n",
      "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (conv2): Convolution(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (norm1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (2): UnetBasicBlock(\n",
      "      (conv1): Convolution(\n",
      "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (conv2): Convolution(\n",
      "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (norm1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (norm2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (3): UnetBasicBlock(\n",
      "      (conv1): Convolution(\n",
      "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (conv2): Convolution(\n",
      "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (norm1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (norm2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "  )\n",
      "  (bottleneck): UnetBasicBlock(\n",
      "    (conv1): Convolution(\n",
      "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (conv2): Convolution(\n",
      "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (norm1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (norm2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "  )\n",
      "  (upsamples): ModuleList(\n",
      "    (0): UnetUpBlock(\n",
      "      (transp_conv): Convolution(\n",
      "        (conv): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "      )\n",
      "      (conv_block): UnetBasicBlock(\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        (norm1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "        (norm2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (1): UnetUpBlock(\n",
      "      (transp_conv): Convolution(\n",
      "        (conv): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "      )\n",
      "      (conv_block): UnetBasicBlock(\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        (norm1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "        (norm2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (2): UnetUpBlock(\n",
      "      (transp_conv): Convolution(\n",
      "        (conv): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "      )\n",
      "      (conv_block): UnetBasicBlock(\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        (norm1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "        (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (3): UnetUpBlock(\n",
      "      (transp_conv): Convolution(\n",
      "        (conv): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "      )\n",
      "      (conv_block): UnetBasicBlock(\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        (norm1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "        (norm2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (4): UnetUpBlock(\n",
      "      (transp_conv): Convolution(\n",
      "        (conv): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "      )\n",
      "      (conv_block): UnetBasicBlock(\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        (norm1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "        (norm2): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_block): UnetOutBlock(\n",
      "    (conv): Convolution(\n",
      "      (conv): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (skip_layers): DynUNetSkipLayer(\n",
      "    (downsample): UnetBasicBlock(\n",
      "      (conv1): Convolution(\n",
      "        (conv): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (conv2): Convolution(\n",
      "        (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (norm1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (norm2): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (next_layer): DynUNetSkipLayer(\n",
      "      (downsample): UnetBasicBlock(\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        (norm1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "        (norm2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      )\n",
      "      (next_layer): DynUNetSkipLayer(\n",
      "        (downsample): UnetBasicBlock(\n",
      "          (conv1): Convolution(\n",
      "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          )\n",
      "          (conv2): Convolution(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          )\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          (norm1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "        )\n",
      "        (next_layer): DynUNetSkipLayer(\n",
      "          (downsample): UnetBasicBlock(\n",
      "            (conv1): Convolution(\n",
      "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            )\n",
      "            (conv2): Convolution(\n",
      "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            )\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "            (norm1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (norm2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          )\n",
      "          (next_layer): DynUNetSkipLayer(\n",
      "            (downsample): UnetBasicBlock(\n",
      "              (conv1): Convolution(\n",
      "                (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              )\n",
      "              (conv2): Convolution(\n",
      "                (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              )\n",
      "              (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "              (norm1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "              (norm2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            )\n",
      "            (next_layer): UnetBasicBlock(\n",
      "              (conv1): Convolution(\n",
      "                (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              )\n",
      "              (conv2): Convolution(\n",
      "                (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              )\n",
      "              (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "              (norm1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "              (norm2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            )\n",
      "            (upsample): UnetUpBlock(\n",
      "              (transp_conv): Convolution(\n",
      "                (conv): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "              )\n",
      "              (conv_block): UnetBasicBlock(\n",
      "                (conv1): Convolution(\n",
      "                  (conv): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (conv2): Convolution(\n",
      "                  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "                (norm1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "                (norm2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (upsample): UnetUpBlock(\n",
      "            (transp_conv): Convolution(\n",
      "              (conv): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "            )\n",
      "            (conv_block): UnetBasicBlock(\n",
      "              (conv1): Convolution(\n",
      "                (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              )\n",
      "              (conv2): Convolution(\n",
      "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              )\n",
      "              (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "              (norm1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "              (norm2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (upsample): UnetUpBlock(\n",
      "          (transp_conv): Convolution(\n",
      "            (conv): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "          )\n",
      "          (conv_block): UnetBasicBlock(\n",
      "            (conv1): Convolution(\n",
      "              (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            )\n",
      "            (conv2): Convolution(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            )\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "            (norm1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (upsample): UnetUpBlock(\n",
      "        (transp_conv): Convolution(\n",
      "          (conv): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "        )\n",
      "        (conv_block): UnetBasicBlock(\n",
      "          (conv1): Convolution(\n",
      "            (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          )\n",
      "          (conv2): Convolution(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          )\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          (norm1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (norm2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (upsample): UnetUpBlock(\n",
      "      (transp_conv): Convolution(\n",
      "        (conv): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "      )\n",
      "      (conv_block): UnetBasicBlock(\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        (norm1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "        (norm2): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_normal_per_module(from_module: nn.Module, to_module: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Initialize the weights of a model (to_module) using the normal distributions of the layers in another module (from_module).\n",
    "    The function will take the weights in an entire module i.e. shape [128, 64, 3, 3] and compute a single number mean and std to \n",
    "    init the entire layer in another module.\n",
    "    \n",
    "    Args:\n",
    "        from_module (nn.Module): Source module providing weights for initialization.\n",
    "        to_module (nn.Module): Target module whose weights will be initialized.\n",
    "    \n",
    "    Note:\n",
    "        The modules should be -very- similar as this functions makes the assumption the models have the same layers (either 2D or 3D) in the same order.\n",
    "    \"\"\"\n",
    "    assert len(list(from_module.modules())) == len(list(to_module.modules())), \"Error: Models should contain the 'same' layers\"\n",
    "\n",
    "    for from_mod, to_mod in zip(from_module.modules(), to_module.modules()):\n",
    "\n",
    "        if isinstance(from_mod, (nn.Conv2d, nn.InstanceNorm2d, nn.ConvTranspose2d)):\n",
    "            # Get distributions across the entire module, i.e. a single number for mean and std\n",
    "            mean = torch.mean(from_mod.weight.data) \n",
    "            std  = torch.std(from_mod.weight.data)\n",
    "            \n",
    "            to_mod.weight.data.normal_(mean, std)\n",
    "\n",
    "        elif isinstance(from_mod, nn.LeakyReLU):\n",
    "            to_mod.negative_slope = from_mod.negative_slope\n",
    "\n",
    "init_normal_per_module(model2d, model3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_normal_per_layer(from_module: nn.Module, to_module: nn.Module) -> None:\n",
    "    pass\n",
    "\n",
    "init_normal_per_layer(model2d, model3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_mod.weight.data = torch.ones(to_mod.weight.data.shape)#torch.normal(mean.expand(weight_shape), std.expand(weight_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init from \"layer-wise\"\n",
    "a = torch.randn([32, 32, 3, 3])\n",
    "torch.mean(a, axis=[0,1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.],\n",
      "           [1., 1., 1.],\n",
      "           [1., 1., 1.]]]]])\n"
     ]
    }
   ],
   "source": [
    "for mod in model3d.modules():\n",
    "    if isinstance(mod, (nn.Conv3d, nn.InstanceNorm3d, nn.ConvTranspose3d)):\n",
    "        print(mod.weight.data)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5979])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(10,1)\n",
    "torch.normal(torch.zeros(1), std = torch.ones(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(1).expand(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
