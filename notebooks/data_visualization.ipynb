{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "# from tqdm import tqdm\n",
    "import medpy.metric as metric\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "import monai\n",
    "from monai.data import DataLoader, Dataset\n",
    "from monai.transforms.utils import allow_missing_keys_mode\n",
    "from monai.transforms import BatchInverseTransform\n",
    "from monai.networks.nets import DynUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "seed = 0\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "monai.utils.misc.set_determinism(seed, use_deterministic_algorithms=True)\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_kits_data(dataset_path: str = \"/dtu/3d-imaging-center/courses/02510/data/KiTS19\") -> list[dict]:\n",
    "    \"\"\" Assumes the data has quite specific structure. \"\"\"\n",
    "    return [\n",
    "        {\n",
    "            'image': str(path),                                                         # the unlabeled volume file, 'imaging.nii.gz'\n",
    "            'label': str(path).replace(\"imaging.nii.gz\", \"segmentation_kidney.nii.gz\"), # all the labeled data is named 'segmentation_kidney.nii.gz'\n",
    "            'id': path.parent.name                                                      # the dir name, e.g. 'case_00000'\n",
    "        }\n",
    "        for path in sorted(Path(dataset_path).glob(\"*/imaging.nii.gz\"))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_acd_data(dataset_path: str = \"/dtu/3d-imaging-center/courses/02510/data/ACDC17\") -> list[dict]:\n",
    "    \"\"\" Assumes the data is in patient00X folders etc. \"\"\"\n",
    "    return [\n",
    "        {\n",
    "            'image': str(path),                                 # the unlabeled volume file\n",
    "            'label': str(path).replace(\".nii.gz\", \"_gt.nii.gz\"),# all the labeled data has _gt in the path\n",
    "            'id': path.name.split(\".\")[0]                       # the file name without ending \n",
    "        }\n",
    "        for path in sorted(Path(dataset_path).glob(\"*/*frame[0-9][0-9].nii.gz\"))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': '/dtu/3d-imaging-center/courses/02510/data/KiTS19/case_00000/imaging.nii.gz', 'label': '/dtu/3d-imaging-center/courses/02510/data/KiTS19/case_00000/segmentation_kidney.nii.gz', 'id': 'case_00000'}\n"
     ]
    }
   ],
   "source": [
    "all_files = read_kits_data()\n",
    "print(all_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "im = nib.load(all_files[0]['image'])\n",
    "label = nib.load(all_files[0]['label'])\n",
    "name = all_files[0]['id']\n",
    "\n",
    "x, y, z = im.shape[0] // 2, im.shape[1] // 2, im.shape[2] // 2\n",
    "\n",
    "plot = nib.viewers.OrthoSlicer3D(label.get_fdata(), title=\"Center slice of MRI data\")\n",
    "plot.set_position(x,y,z)\n",
    "plt.tight_layout()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Get the (new) voxel size\n",
    "# voxel_sizes = np.array([nib.load(file['image']).header.get_zooms() for file in all_files])\n",
    "voxel_dims = 1.464845, 1.464845, 10.0 # np.median(voxel_sizes, axis=0)\n",
    "\n",
    "n_classes = 2\n",
    "im_dims = 256, 256,-1 #TODO: Should be defined further up\n",
    "batch_size = 4 # TODO: Batch size >= 1 doesnt work for some reason\n",
    "\n",
    "transforms = monai.transforms.Compose([\n",
    "    monai.transforms.LoadImaged(keys=['image', 'label']),\n",
    "    monai.transforms.EnsureChannelFirstd(keys=['image', 'label']),\n",
    "    monai.transforms.Spacingd(keys=['image', 'label'], pixdim=voxel_dims, mode=[\"bilinear\", \"nearest\"]),\n",
    "    monai.transforms.ResizeWithPadOrCropd(keys=['image', 'label'], spatial_size=im_dims),\n",
    "    monai.transforms.RandSpatialCropd(keys=['image', 'label'], roi_size=[-1, -1, 1]),\n",
    "    # monai.transforms.SqueezeDimd(keys=['image', 'label'], dim=-1), # TODO: This doesn't work for some reason\n",
    "\n",
    "    # TODO: (Light) data augmentation\n",
    "\n",
    "    # monai.transforms.AsDiscreted(keys=['label'], to_onehot=n_classes) # Convert \"label\" to onehot encoded.\n",
    "])\n",
    "\n",
    "full_dataset = Dataset(data=all_files, transform=transforms)\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, lengths = [0.7, 0.1, 0.2])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader   = DataLoader(val_dataset, batch_size=1, shuffle=False) \n",
    "test_dataloader  = DataLoader(test_dataset, batch_size=1, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num params: 20_611_170\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model): \n",
    "    \"\"\" Get the number of params in a model. See: https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "model = DynUNet(\n",
    "    spatial_dims = 2,   # 2 for 2D convolutions, 3 for 3D convolutions\n",
    "    in_channels  = 1,   # Number of input channels/modalities (3 for RGB)\n",
    "    out_channels = n_classes,   # Number of classes, including background\n",
    "    kernel_size  = [3, 3, 3, 3, 3, 3], # Size of the filters\n",
    "    strides      = [1, 2, 2, 2, 2, 2],\n",
    "    upsample_kernel_size = [2, 2, 2, 2, 2]\n",
    ").to(device)\n",
    "\n",
    "print(f\"Num params: {count_parameters(model):_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "torch.Size([1, 2, 256, 256])\n",
      "metatensor(13.6934)\n",
      "0.004202576362030636\n"
     ]
    }
   ],
   "source": [
    "loss_fn = monai.losses.DiceLoss(softmax=True, to_onehot_y=False) # Apply \"softmax\" to the output of the network and don't convert to onehot because this is done already by the transforms.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "inferer = monai.inferers.SliceInferer(roi_size=[-1, -1], spatial_dim=2, sw_batch_size=1)\n",
    "\n",
    "with torch.no_grad():  # Do not need gradients for this part\n",
    "    for batch in val_dataloader:\n",
    "        x = batch['image'].to(device).squeeze(dim = -1) # TODO: This squeeze has to be here because monai.transforms.SqueezeDimd gives error\n",
    "        y = batch['label'].to(device).squeeze(dim = -1) # TODO: This squeeze has to be here because monai.transforms.SqueezeDimd gives error\n",
    "        \n",
    "        print(x.shape, y.shape)\n",
    "        pred = model(x)\n",
    "        print(pred.shape)\n",
    "        # print(pred.shape)\n",
    "        # prediction = inferer(inputs=x, network=model)\n",
    "        print(pred.max())\n",
    "        # print(metric.dc(pred.argmax(dim=1), y))\n",
    "        print(metric.dc(torch.ones_like(y), y))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NJKNASJKD\n"
     ]
    }
   ],
   "source": [
    "if torch.count_nonzero(torch.ones(1, 2, 256, 256, dtype=torch.int32)):\n",
    "    print(\"NJKNASJKD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The first supplied array does not contain any binary object.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# torch.Size([1, 2, 256, 256]) torch.Size([1, 2, 256, 256])\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhd95\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deeplearning3d/2d-to-3d-transfer-learning/.env/lib/python3.10/site-packages/medpy/metric/binary.py:413\u001b[0m, in \u001b[0;36mhd95\u001b[0;34m(result, reference, voxelspacing, connectivity)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhd95\u001b[39m(result, reference, voxelspacing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, connectivity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    372\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    95th percentile of the Hausdorff Distance.\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;124;03m    This is a real metric. The binary images can therefore be supplied in any order.\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m     hd1 \u001b[38;5;241m=\u001b[39m \u001b[43m__surface_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvoxelspacing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnectivity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m     hd2 \u001b[38;5;241m=\u001b[39m __surface_distances(reference, result, voxelspacing, connectivity)\n\u001b[1;32m    415\u001b[0m     hd95 \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mpercentile(numpy\u001b[38;5;241m.\u001b[39mhstack((hd1, hd2)), \u001b[38;5;241m95\u001b[39m)\n",
      "File \u001b[0;32m~/deeplearning3d/2d-to-3d-transfer-learning/.env/lib/python3.10/site-packages/medpy/metric/binary.py:1265\u001b[0m, in \u001b[0;36m__surface_distances\u001b[0;34m(result, reference, voxelspacing, connectivity)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;66;03m# test for emptiness\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m==\u001b[39m numpy\u001b[38;5;241m.\u001b[39mcount_nonzero(result):\n\u001b[0;32m-> 1265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1266\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe first supplied array does not contain any binary object.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1267\u001b[0m     )\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m==\u001b[39m numpy\u001b[38;5;241m.\u001b[39mcount_nonzero(reference):\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1270\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe second supplied array does not contain any binary object.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1271\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The first supplied array does not contain any binary object."
     ]
    }
   ],
   "source": [
    "# torch.Size([1, 2, 256, 256]) torch.Size([1, 2, 256, 256])\n",
    "metric.binary.hd95(torch.zeros(1, 2, 256, 256, dtype=torch.int32).numpy(), torch.ones(1, 2, 256, 256, dtype=torch.int32).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Epoch 1/3 ****\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 1, 256, 256, 28]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[194], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m y \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 24\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y)\n\u001b[1;32m     27\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/dtu/3d-imaging-center/courses/conda/miniconda3/envs/env-02510/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dtu/3d-imaging-center/courses/conda/miniconda3/envs/env-02510/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/dtu/3d-imaging-center/courses/conda/miniconda3/envs/env-02510/lib/python3.11/site-packages/monai/networks/nets/dynunet.py:269\u001b[0m, in \u001b[0;36mDynUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 269\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskip_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_block(out)\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeep_supervision:\n",
      "File \u001b[0;32m/dtu/3d-imaging-center/courses/conda/miniconda3/envs/env-02510/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dtu/3d-imaging-center/courses/conda/miniconda3/envs/env-02510/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/dtu/3d-imaging-center/courses/conda/miniconda3/envs/env-02510/lib/python3.11/site-packages/monai/networks/nets/dynunet.py:47\u001b[0m, in \u001b[0;36mDynUNetSkipLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 47\u001b[0m     downout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     nextout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_layer(downout)\n\u001b[1;32m     49\u001b[0m     upout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample(nextout, downout)\n",
      "File \u001b[0;32m/dtu/3d-imaging-center/courses/conda/miniconda3/envs/env-02510/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dtu/3d-imaging-center/courses/conda/miniconda3/envs/env-02510/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/dtu/3d-imaging-center/courses/conda/miniconda3/envs/env-02510/lib/python3.11/site-packages/monai/networks/blocks/dynunet_block.py:171\u001b[0m, in \u001b[0;36mUnetBasicBlock.forward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inp):\n\u001b[0;32m--> 171\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(out)\n\u001b[1;32m    173\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlrelu(out)\n",
      "File \u001b[0;32m/dtu/3d-imaging-center/courses/conda/miniconda3/envs/env-02510/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dtu/3d-imaging-center/courses/conda/miniconda3/envs/env-02510/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/dtu/3d-imaging-center/courses/conda/miniconda3/envs/env-02510/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/dtu/3d-imaging-center/courses/conda/miniconda3/envs/env-02510/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dtu/3d-imaging-center/courses/conda/miniconda3/envs/env-02510/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/dtu/3d-imaging-center/courses/conda/miniconda3/envs/env-02510/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dtu/3d-imaging-center/courses/conda/miniconda3/envs/env-02510/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dtu/3d-imaging-center/courses/conda/miniconda3/envs/env-02510/lib/python3.11/site-packages/monai/data/meta_tensor.py:282\u001b[0m, in \u001b[0;36mMetaTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 282\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# if \"out\" in kwargs:\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m#     return ret\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _not_requiring_metadata(ret):\n",
      "File \u001b[0;32m/dtu/3d-imaging-center/courses/conda/miniconda3/envs/env-02510/lib/python3.11/site-packages/torch/_tensor.py:1386\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m   1385\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m-> 1386\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1387\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   1388\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 1, 256, 256, 28]"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "n_epochs = 3\n",
    "\n",
    "loss_fn = monai.losses.DiceLoss(softmax=True, to_onehot_y=False) # Apply \"softmax\" to the output of the network and don't convert to onehot because this is done already by the transforms.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "inferer = monai.inferers.SliceInferer(roi_size=[-1, -1], spatial_dim=2, sw_batch_size=1)\n",
    "\n",
    "iterations = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"**** Epoch {epoch+1}/{n_epochs} ****\")\n",
    "\n",
    "    training_loss = 0.0\n",
    "    validation_loss = 0.0\n",
    "\n",
    "    # Train\n",
    "    model.train()\n",
    "    for batch_num, batch in enumerate(train_dataloader):\n",
    "        x = batch['image'].to(device)\n",
    "        y = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(x)\n",
    "\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        training_loss += loss\n",
    "\n",
    "        if (iterations % config.wandb.train_log_interval == config.wandb.train_log_interval - 1):\n",
    "            print(f\"{batch_num + 1}/{len(train_dataloader)} | loss: {loss:.3f}\")\n",
    "            wandb.log({\n",
    "                \"epoch\" : epoch,\n",
    "                \"iteration\" : iterations,\n",
    "                \"batch/train\" : batch_num,\n",
    "                \"loss/train\"  : training_loss.item() / (batch_num + 1),\n",
    "            })\n",
    "        \n",
    "        iterations += 1\n",
    "\n",
    "\n",
    "    \n",
    "    # Validate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_num, batch in enumerate(val_dataloader):\n",
    "            x = batch['image'].to(device)\n",
    "            y = batch['label'].to(device)\n",
    "\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            validation_loss += loss\n",
    "\n",
    "            if (batch_num % config.wandb.validation_log_interval == config.wandb.validation_log_interval - 1):\n",
    "                print(f\"{batch_num + 1}/{len(val_dataloader)} | val loss: {loss.item():.3f}\")\n",
    "                wandb.log({\n",
    "                    \"epoch\" : epoch,\n",
    "                    \"batch/val\" : batch_num,\n",
    "                    \"loss/val\": validation_loss.item() / (batch_num + 1),\n",
    "                })\n",
    "\n",
    "\n",
    "    \n",
    "    # Logging\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
