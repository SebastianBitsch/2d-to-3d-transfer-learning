# This is the default config that all other configs for doing individual experiments will
# inherit from. If values aren't overwritten they grab their values from here.

base:
  experiment_name: base # Should be overwriten in exp files
  save_interval: 1000 # how often should the weights of the model be saved
  save_location: /work3/s204163/3dimaging_finalproject/weights # will be saved as {experiment_name}_{n_iterations}

model:
  path_to_weights: ~  # if not None will continue training from these weights. path to the weights, should be in the /models/ dir
  num_dimensions: 2   # 2 or 3 - for 2D or 3D nothing else pls
  kernel_size: [3, 3, 3, 3, 3, 3]
  strides: [1, 2, 2, 2, 2, 2]
  upsample_kernel_size: [2, 2, 2, 2, 2]

data:
  dataset_name: kits # only kits and adc supported
  dataset_path: /dtu/3d-imaging-center/courses/02510/data/KiTS19
  use_dataset_a: true # the dataset is split into a and b dataset - a used for pretraining b used for finetuning
  n_classes: 2
  image_dims: [256, 256, -1] # what size should the images/volumes be resized to
  voxel_dims: [1.464845, 1.464845, 10.0] # what size should the voxels be resized to. this value is found in a notebook beforehand
  crop_size: [-1, -1, 1] # whether to crop the 3D volume to a 2d slice, if [-1,-1,1] will crop. if [-1,-1,-1] wont crop. kinda shitty way of doing it

  num_workers: 0       # faster with 0 workers on my mac m1 idk why (7 seconds vs 30 seconds per epoch training)
  pct_train_split: 0.8 # of the half data set
  pct_val_split  : 0.2 # of the half data set
  pct_test_split : 0.2 # of the full data set

hyperparameters:
  seed: 1312
  batch_size: 4
  epochs: 200_000 # high number so training time is the limiting factor
  learning_rate: 1e-3
  use_scheduler: false      # if using scheduler the lr will switch to 1e-5 after half the training time has passed
  max_num_iterations: 200_000 # (high number so training time is the limiting factor) how many iterations to run / stop at (if n epochs and max time is large enough)
  max_training_time: 7200 # in seconds, how long to train for (if max_iterations and n epochs is large enough)


wandb:
  mode: disabled # "disabled" or "online" or "offline"
  team_name: deeplearning_3d
  project_name: transfer-learning-2d-3d
  run_name: ~ # Set this per experiment to name the run something nice. if not set it will give some shitty name like vibrant-meadow
  train_log_interval: 5
  validation_log_interval: 5